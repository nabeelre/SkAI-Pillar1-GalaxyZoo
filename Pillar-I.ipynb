{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e6a87",
   "metadata": {
    "id": "2d6e6a87"
   },
   "source": [
    "## Overview\n",
    "**Adam?** Two sentences on Pillar I, what the intention of this activity is ...\n",
    "\n",
    "\n",
    "### Galaxy Zoo Project\n",
    "Galaxy Zoo is a citizen science project that enlists volunteers to visually classify galaxies based on their morphology using images from various telescopes ([Lintott et al., 2008](https://doi.org/10.1111/j.1365-2966.2008.13689.x)). With immense contributions from citizen scientists, this effort has yielded large, labeled data sets well suited for large-scale data analysis in astronomy. We will work with Galaxy Zoo 2, which involves classified images of ~240,000 galaxies from the Sloan Digital Sky Survey ([Willett et al., 2013](https://doi.org/10.1093/mnras/stt1458); [Hart et al., 2016](https://academic.oup.com/mnras/article/461/4/3663/2608720)).\n",
    "\n",
    "### Swin Transformer\n",
    "We will process the galaxy images with a **Swin Transformer**, a hierarchical vision transformer model that computes self-attention within shifted windows, achieving state-of-the-art performance on computer vision tasks ([Liu et al., 2021](https://arxiv.org/abs/2103.14030)). Its design enables efficient computation and scalability to various image resolutions, making it suitable for diverse applications.\n",
    "\n",
    "Today, we'll work with a Swin Transformer pretrained on the ImageNet dataset: a large-scale image database containing millions of labeled images across 1,000 categories like cats, dogs, oil filters, and balloons. Although a model trained on such images may not appear useful for galaxy classification, we'll find that this pre-trained Swin Transformer is a generally effective tool for extracting information from images.\n",
    "\n",
    "### Embeddings\n",
    "Nabeel and Ved have run all ~240,000 Galaxy Zoo 2 images through this pre-trained Swin Transformer, producing **embeddings**. These embeddings are dense 1D vector representations of the data, ideally capturing all useful information in the image in a low-dimensional space.\n",
    "\n",
    "### Let's start with loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "JaxbsgSflmeA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1746570540223,
     "user": {
      "displayName": "Nabeel Rehemtulla",
      "userId": "11522271869226078312"
     },
     "user_tz": 300
    },
    "id": "JaxbsgSflmeA",
    "outputId": "d2092ba1-5bb1-4d5a-dd36-6af16c9b5329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# You can either run this notebook in Google Colab and grant it access to files in your Google Drive.\n",
    "# To do so, you'll first need to add a shortcut to this Google Drive folder to your own Drive\n",
    "  # Open https://drive.google.com/drive/folders/176O-BZ6OcNAAmruBdk32PQLdp4iaahy5?usp=share_link\n",
    "  # Click on SkAI-Pillar-1-GalaxyZoo\n",
    "  # Click on Organize -> Add Shortcut\n",
    "  # Add it to My Drive/\n",
    "# Then, load the Drive helper and mount\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"drive/My Drive/SkAI-Pillar-1-GalaxyZoo\")\n",
    "\n",
    "# Alternatively, you can clone https://github.com/nabeelre/SkAI-Pillar1-GalaxyZoo\n",
    "# You'll then need to install all the necessary packages locally and download the data file\n",
    "# from https://drive.google.com/file/d/1_7-HlbLUZbgQzT5iaaVk0kScTCyYQ9kv/view?usp=share_link\n",
    "# and the images from https://drive.google.com/file/d/1ax_lgZTsaKyFCD-E1Dax7ywuX74bd0Ca/view?usp=share_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce9939",
   "metadata": {
    "executionInfo": {
     "elapsed": 34827,
     "status": "ok",
     "timestamp": 1746570342520,
     "user": {
      "displayName": "Nabeel Rehemtulla",
      "userId": "11522271869226078312"
     },
     "user_tz": 300
    },
    "id": "a6ce9939"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import umap.umap_ as umap\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def show_image_by_assetid(asset_id, images_dir=\"data/3565489/images/\", ax=None):\n",
    "    file_path = os.path.join(images_dir, f\"{asset_id}.jpg\")\n",
    "    if os.path.exists(file_path):\n",
    "        img = Image.open(file_path)\n",
    "\n",
    "        if ax is not None:\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e313c90",
   "metadata": {
    "id": "4e313c90"
   },
   "outputs": [],
   "source": [
    "gz_df = pd.read_csv('data/gz2_data.csv', index_col=None)\n",
    "\n",
    "gz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68af848",
   "metadata": {
    "id": "b68af848"
   },
   "outputs": [],
   "source": [
    "embs = gz_df[[f'embedding_{i}' for i in range(768)]]\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b4eaf",
   "metadata": {
    "id": "458b4eaf"
   },
   "source": [
    "This `DataFrame` contains information on the 240,000 galaxies classified in the Galaxy Zoo 2 project. Notable columns including the following\n",
    "\n",
    "`morphology` is a string describing the shape/type of the galaxy. For astronomers, these are similar to but not identical to the standard Hubble Tuning Fork types. Morphologies beginning with `E` are galaxies with smooth light profiles. Their degree of roundless is encoded by the following character: `r` for completely round, `i` for in-between, and `c` for cigar-shaped. Morphologies beginning with `S` are galaxies with disks. Edge-on disks follow `S` with `er`, `eb`, or `en` for having a round, boxy, or no bulge respectively. Face-on disks have `B` following `S` if they show a bar and `a`, `b`, `c`, `d` next encoding the prominance of the bulge: dominant, obvious, just noticable, and none, respectively.\n",
    "\n",
    "`REDSHIFT` is a float containing the spectroscopic redshift of the galaxy. In simplest terms, redshift ($z$) is a measure of distance from us where larger values indicate greater distances and a minimum at $z=0$, indicating no distance from us.  \n",
    "\n",
    "Where relevant, Galaxy Zoo citizen scientists also indicate additional information, like the presence of a ring shape (`ring`), an arc resulting from lensing (`lens/arc`), a galaxy with disturbed morphology (`disturbed`), irregular morphology (`irregular`), a galaxy merger (`merger`), a prominent dust lane (`dust lane`), or something else (`other`). For face-on disk galaxies, the count of spiral arms is available (`arm_count`; `+` indicates more than 4 arms) and how tightly they are wound (`arm_winding`; tightly `t`, moderately `m`, and loosely `l`).\n",
    "\n",
    "`asset_id` can be used for fetching the image of a galaxy. We've provided a helpful `show_image_by_assetid()` function.\n",
    "\n",
    "`embedding_0` to `embedding_767` represent the 1D embedding vector we produced with the pre-trained Swin transformer. We've extracted these into a 2D array where each row is an embedding that corresponds to the galaxy in the same row of `gz_df`.\n",
    "\n",
    "Many other features are available like the brightness of the galaxy in `UGRIZ` bands are also available. We also include the sky coordinates and `SDSS` DR7 object IDs in case you'd like to gather other information on these galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3ed9c",
   "metadata": {
    "id": "9eb3ed9c"
   },
   "source": [
    "### Let's start by looking at some examples in each morphology type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47096575",
   "metadata": {
    "id": "47096575"
   },
   "outputs": [],
   "source": [
    "morphologies = ['Er', 'Ei', 'Ec']\n",
    "filtered_dfs = {morph: gz_df[gz_df['morphology'] == morph].sample(5) for morph in morphologies}\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "\n",
    "for col, morph in enumerate(morphologies):\n",
    "    for row, (_, row_data) in enumerate(filtered_dfs[morph].iterrows()):\n",
    "        ax = axes[row, col]\n",
    "        show_image_by_assetid(row_data['asset_id'], ax=ax)\n",
    "        ax.set_title(f\"{morph} - asset_id={row_data['asset_id']}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Smooth galaxies with round, intermediate, and cigar shapes', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af89e2c",
   "metadata": {
    "id": "2af89e2c"
   },
   "outputs": [],
   "source": [
    "morphologies = ['Ser', 'Seb', 'Sen']\n",
    "filtered_dfs = {morph: gz_df[gz_df['morphology'] == morph].sample(5) for morph in morphologies}\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "\n",
    "for col, morph in enumerate(morphologies):\n",
    "    for row, (_, row_data) in enumerate(filtered_dfs[morph].iterrows()):\n",
    "        ax = axes[row, col]\n",
    "        show_image_by_assetid(row_data['asset_id'], ax=ax)\n",
    "        ax.set_title(f\"{morph} - asset_id={row_data['asset_id']}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Disk galaxies with round, boxy, and no bulges', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fb215",
   "metadata": {
    "id": "b84fb215"
   },
   "outputs": [],
   "source": [
    "morphologies = ['SBa', 'SBd', 'Sa', 'Sd']\n",
    "filtered_dfs = {morph: gz_df[gz_df['morphology'] == morph].sample(5) for morph in morphologies}\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(15, 20))\n",
    "\n",
    "for col, morph in enumerate(morphologies):\n",
    "    for row, (_, row_data) in enumerate(filtered_dfs[morph].iterrows()):\n",
    "        ax = axes[row, col]\n",
    "        show_image_by_assetid(row_data['asset_id'], ax=ax)\n",
    "        ax.set_title(f\"{morph} - asset_id={row_data['asset_id']}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Disk galaxies with/without bars (B / no B) with prominent/weak bulges (a/d)', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a43797",
   "metadata": {
    "id": "32a43797"
   },
   "outputs": [],
   "source": [
    "flags = ['ring', 'lens/arc', 'merger', 'disturbed', 'other']\n",
    "filtered_rows = [gz_df[gz_df[flag] == 1].sample(1).iloc[0] for flag in flags]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for ax, flag, row_data in zip(axes, flags, filtered_rows):\n",
    "    show_image_by_assetid(row_data['asset_id'], ax=ax)\n",
    "    ax.set_title(f\"{flag.capitalize()} - asset_id={row_data['asset_id']}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Galaxies with some special features', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999c1b8",
   "metadata": {
    "id": "3999c1b8"
   },
   "source": [
    "### Let's see if we can see any clustering in the embeddings!\n",
    "The embeddings should contain much of the information in those images. Inspecting how the embeddings cluster is impossible in their native 768 dimensional space, so we'll have to reduce their dimensionality. We'll start this off with [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection), a dimensionality reduction technique that preserves the global structure of data while emphasizing local relationships.\n",
    "\n",
    "You can also try out [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), and other similar tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdedca",
   "metadata": {
    "id": "67bdedca"
   },
   "outputs": [],
   "source": [
    "# Run UMAP on the embeddings\n",
    "umap_model = umap.UMAP()\n",
    "embedding_2d = umap_model.fit_transform(embs)\n",
    "\n",
    "# store the embeddings into our dataframe\n",
    "gz_df[['UMAP_1', 'UMAP_2']] = embedding_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65885138",
   "metadata": {
    "id": "65885138"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(gz_df['UMAP_1'], gz_df['UMAP_2'], s=5)\n",
    "plt.xlabel('UMAP_1')\n",
    "plt.ylabel('UMAP_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6300c",
   "metadata": {
    "id": "b2e6300c"
   },
   "source": [
    "### Possibile tasks with Galaxy Zoo 2 embeddings\n",
    "1. Galaxy morphology classification\n",
    "\n",
    "2. Anomalous galaxy detection\n",
    "\n",
    "3. (*Challenge*) Fine-tuning a pre-trained model to compete with state-of-the-art performance\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [Cao et al 2024](https://www.aanda.org/articles/aa/pdf/2024/03/aa48544-23.pdf) claims ~98% accuracy at 5-way classification on this dataset with a convolutional vision transformer.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;... Or whatever else you can think of!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "BTSbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
